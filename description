Reponsibilities:

    Implement near real-time ETL-like processes from hundreds of applications and data sources using the Apache Kafka ecosystem of technologies.
    Designing, developing, testing and tuning a large-scale ‘stream data platform’ for connecting systems across our business in a decoupled manner.
    Deliver data in near real-time from transactional data stores into analytical data stores.
    R&D ways to acquire data and suggest new uses for that data.
    “Stream processing.”  Enable applications to react to, process and transform streams of data between business domains.
    “Data Integration.”  Capture application events and data store changes and pipe to other interested systems.

Requirements:
    Proficient in a functional programming language such as Scala, Clojure, Erlang, Elixir, or F# (we’re using Scala).
    Familiarity with JVM languages such as Java, Scala, or Clojure.
    Comfortable working in both Linux and Windows environments.  Our systems all run on Linux, but we interact with many systems running on Windows servers.
    Shell scripting & common Linux tool skills.
    Experience with build tools such as Maven, sbt, or rake.
    Knowledge of distributed systems.
    Knowledge of, or experience with, Kafka a plus.
    Knowledge of Event-Driven/Reactive systems.
    You dig:
        Stream processing tools (Kafka Streams, Storm, Spark, Flink, Google Cloud DataFlow etc.)
        SQL-based technologies (SQL Server, MySQL, PostgreSQL, etc.)
        NoSQL technologies (Cassandra, MongoDB, Redis, HBase, etc.)
        Server automation tools (Ansible, Chef, Puppet, Vagrant, etc.)
        Distributed Source Control (Mercurial, Git)
        The Cloud (Azure, Amazon AWS)
        The ELK Stack (Elasticsearch, Logstash, Kibana)
    Experience with DevOps practices like Continuous Integration, Continuous Deployment, Build Automation, Server automation and Test Driven Development.

